\documentclass[a4paper]{article}
\usepackage{graphicx}

\input{style/head.tex}

%-------------------------------
%	TITLE VARIABLES (identify your work!)
%-------------------------------
%-------------------------------
%	TITLE VARIABLES (identify your work!)
%-------------------------------

\newcommand{\yourname}{Philippe BAZET} % replace YOURNAME with your name
\newcommand{\youremail}{pbazet@yahoo.com} % replace YOUREMAIL with your email
\newcommand{\assignmentnumber}{2} % replace X with the lab session number

\begin{document}

%-------------------------------
%	TITLE SECTION (do not modify unless you really need to)
%-------------------------------
\input{style/header.tex}

%-------------------------------
%	ASSIGNMENT CONTENT (add your responses)
%-------------------------------

\section{Question 1}
As mention in \cite{luong2016}, greedy decoding stragegy is "heavily" suboptimal, two alternative strategies are proposed
\begin{itemize}
\item Beam Research. At each step, instead of one single hypothesis, a fixed set of $K$ possibles translations are tested in parallel. From each of these $K$ possible translations, news probability are built for next new word. From this newly built hypothesis set, only $K$ most probable sentences are selected
\item Ensemble Research. An ensemble of different statistical learners are run separatly and final solution in made on building new proability by aggregating through predefined operators proability  from different learners
\end{itemize}

\subsection{Sub optimality for greedy research}

In greedy research, a local optimal solution is chosen. The one that choose the most probable word given already generated sentence. Meanwhile, the objective function is global: not select the highest possible next word but the whole sentence that is most probable. The higuest most probable  sentence may include word that are not most probable given words preceding a given word.

Global Objective function for given sentence $X$ translated from sentence $Y$ using notation from \citep{luong2016}
\begin{equation}
Objective_{global}(X) = Max Log (P( X \vert Y)) = \sum_{t} Log( P( x_{t} \vert x_{<t},Y)  
\end{equation}


Greedy objective function
\begin{equation}
Objective_{local}(x_{t}) = Max ( Log( P( x_{t} \vert x_{<t},Y))
\end{equation}


\subsection{Beam Research}
$K$ most probablement sentences $K_{t}$ are tested in parallel at time $t$

When trying to add a new word, probability are estimated for building new sentences from the $K_{t}$ hypothesis test set adding a new word. For each sentence $k_{t} \in K_{t}$, prediction is computed for new word from vocabulary, for $x_{t+1}$ word
\begin{equation}
prediction = Log(P(x_{t+1}^{k} \vert x_{<=t}^{k},Y))
\end{equation}

Prediction are then aggregated (summed) to log probability from $K$ sentences in order to compute probability for hypothesis for new sentences adding new word at end of sentence from $K$ sentences
From this new hypothesis sentences test set, the $K$ most probable sentence are selected for builing new $K_{t+1}$ hypothesis test set 

\subsection{ Ensemble Research}

$M$ concurrent statistical learners are run in parallel. For a given sentence, each learner $m \in M$ will give its probability $p^{m} = P(x_{t}^{m} \vert x_{<t}^{m})$ for next word in sentence. These probabilities are then combined using specific operators to build a probability for next word in sentence using probabilities from $M$ statistical learners. \citep{luong2016} propose two operators
\begin{itemize}
\item Majority Voting (average of probability of different learners)
\item Consensus $(\prod_{m=1}^{M} p^{m})^{1 / M}$

\end{itemize}





\section{Question 2}
Sentences are over translated: same words appear several times in translation. Using approach from \cite{zhaopeng2016}, when translating a new word $x_{t}$, neural translation system should model $C_{t}$ proability that a source word belongs to history of source words that have already been translated. According to \citep{luong2015} attention weights help to give to each source word an importance in the translation for a given target word. As an assumption, the attention weight is correlated to proability that source word has been used for translating target word and thus beeing highly probable. Gor a given step $t$ in translation, in $C_{t}$. \citep{zhaopeng2016} model such historic context with attention weight combined with previous history modeling $C_{t-1}$ and hidden current state $h_{t-1}$ of translation. Additionnaly to model from \citep{luong2015}, attention weight will be also modeled combining history of word being translated $C_{t-1}$
\section{Question 3}

\section{Question 4}
Several phenomenons appear
\begin{itemize}
\item Different meaning for same word "mean".
\item Different grammatical usages for same word "mean" as a verb in first sentence and adjective in seconde sentence.
\item Context sensitivity. Same word "mean" depending on context has very different translation and grammatical properties (meaning and syntax for sample sentences)
\end{itemize}


Thus language model should be able to model different knoweldge level of translated words. 
\begin{itemize}
\item low level pure syntax with phenomenon 2
\item higher semantic level meaning with phenomemon 1
\end{itemize}

Language model should also be able to combine these different levels of knowledge, each level having a different contribution depending of  the language processing task (trasnlation, part of speach tagging etc..).\\

Furthermoe, language model should be able to adapt representation (embedding) of word to context, allowing for same word different representaton depending on context. Not one single context can be associated for a given word. This is a major change from Skip-gram or Continuous Bag of Word model of \cite{quocLe2014doc2vec} first solution to associate to word a rich feature eautomated embedding. Context should embrace the full neighboroud of a word, thus both context words on the left and on the right of a given word\\

Additionnally, for a given task we cannot expect to be able to model all these aspects especially if the availale data set is limited. Thus language model should enable to be re used as a tool box for downstream tasks. Like in \cite{elmo2018}, one approach is to deliver output from a given language model as additional features to be re used possibly by enabling some parameter tuning of offered features. Alternatively as in \cite{bert2018}, the language model can be directly used as the model for downstream task. The language model is pre trained on some denoising tasks (task that artifically damage pre trained coprpus and that are trained to rebuilt the artificially damaged corpus). This pretrained language model can then be plugged in the downstream task. The preset parameters for the pretrained language model being updated during training phase of  the targeted downstream task.



\bibliographystyle{plain}
\bibliography{references} % citation records are in the references.bib document

\end{document}
